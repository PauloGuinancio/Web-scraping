{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_scraping.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Pegar URLS**"
      ],
      "metadata": {
        "id": "Kp0Sd3NKZ7Of"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install selenium==3.141.0\n",
        "\n",
        "!apt-get update # to update ubuntu to correctly run apt install\n",
        "!apt install chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver\n",
        "chrome_options = webdriver.ChromeOptions()\n",
        "chrome_options.add_argument('--headless')\n",
        "chrome_options.add_argument('--no-sandbox')\n",
        "chrome_options.add_argument('--disable-dev-shm-usage')"
      ],
      "metadata": {
        "id": "XK7ZfPYFS4kF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import requests\n",
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "nE6YCR7PbnRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)"
      ],
      "metadata": {
        "id": "LnOtJ16zbqTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Jornal do Comércio**"
      ],
      "metadata": {
        "id": "SsTCBwVObO5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap = 'https://jc.ne10.uol.com.br/sitemap.xml'\n",
        "driver.get(sitemap)\n",
        "response_html = driver.page_source\n",
        "soupHtml = BeautifulSoup(response_html)\n",
        "dontInclude = ['elections',\n",
        "               'author',\n",
        "               'elections2020',\n",
        "               'category',\n",
        "               'autorPost',\n",
        "               'post_tag',\n",
        "               'page-sitemap',\n",
        "               'programming']"
      ],
      "metadata": {
        "id": "ZqsBWF0sfLPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halfway = soupHtml.find_all('loc')\n",
        "halfway = [i.text for i in halfway if 'https' in i.text]\n",
        "print(len(halfway))\n",
        "halfway[0:5]\n",
        "\n"
      ],
      "metadata": {
        "id": "o2wWu4eVfgQ_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap_urls = []\n",
        "for url in halfway:\n",
        "    dummy = []\n",
        "    for dont in dontInclude:\n",
        "        if dont in str(url):\n",
        "            dummy.append(dont)\n",
        "    if dummy == []:\n",
        "        sitemap_urls.append(url)"
      ],
      "metadata": {
        "id": "4-OBKiIWfhr4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getURLs(sitemap_url, driver):\n",
        "    driver.get(sitemap_url)\n",
        "    response_html = driver.page_source\n",
        "    soupHtml = BeautifulSoup(response_html)\n",
        "    urls = soupHtml.find_all('loc')\n",
        "    return [t.text for t in urls if 'http' in t.text]"
      ],
      "metadata": {
        "id": "AFp7yAR8fnsm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def JCURLs(sitemap_urls, verbose=False):\n",
        "    urls = []\n",
        "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "    if verbose:\n",
        "        total = len(sitemap_urls)\n",
        "        progress = 1\n",
        "    for sitemap in sitemap_urls:\n",
        "        if verbose:\n",
        "            print(f'Progress - {round((progress/total)*100,2)}%')\n",
        "            progress+=1\n",
        "        time.sleep(5)\n",
        "        urls = urls + getURLs(sitemap, driver)\n",
        "    driver.quit()\n",
        "    return urls\n"
      ],
      "metadata": {
        "id": "LaO0FdXHfor3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JC = JCURLs(sitemap_urls, verbose=True)"
      ],
      "metadata": {
        "id": "jKMrTJ2jfp8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('JC_urls.txt', 'w') as f:\n",
        "    for url in JC:\n",
        "        f.write(url)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "sqE-wH_ffrCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Brasil de Fato**"
      ],
      "metadata": {
        "id": "IoXcQPElbWLz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response_html = open(r\"brasil-de-fato.txt\")\n",
        "print(response_html)"
      ],
      "metadata": {
        "id": "TjuYIwNEbvKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response_html = driver.page_source\n",
        "soupHtml = BeautifulSoup(response_html)"
      ],
      "metadata": {
        "id": "-vZda5Tlbwli"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(soupHtml.find_all('loc')[1000:1010])\n",
        "\n",
        "urls=[i.text for i in soupHtml.find_all('loc')]\n",
        "\n",
        "print(len(urls))"
      ],
      "metadata": {
        "id": "dYbBya8-bzW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halfway = soupHtml.find_all('url')\n",
        "halfway = soupHtml.find_all('loc')\n",
        "\n",
        "halfway = [i.text  for i in halfway if 'https' in i.text]\n",
        "\n",
        "halfway=halfway[14:]\n",
        "\n",
        "print(halfway[0:50])\n",
        "\n",
        "halfway = list(dict.fromkeys(halfway))\n"
      ],
      "metadata": {
        "id": "Kkz1JSfgb1cu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getURLs(sitemap_url, driver):\n",
        "    driver.get(sitemap_url)\n",
        "    response_html = driver.page_source\n",
        "    soupHtml = BeautifulSoup(response_html)\n",
        "    urls = soupHtml.find_all('url')\n",
        "    urls = soupHtml.find_all('loc')\n",
        "    urls=urls[1:]\n",
        "    return [t.text for t in urls if 'http' in t.text]"
      ],
      "metadata": {
        "id": "C3EECbbCfvvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def BFURLs(sitemap_urls, verbose=False):\n",
        "    urls = []\n",
        "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "    if verbose:\n",
        "        total = len(sitemap_urls)\n",
        "        progress = 1\n",
        "    for sitemap in sitemap_urls:\n",
        "        if verbose:\n",
        "            print(f'Progress - {round((progress/total)*100,2)}%')\n",
        "            progress+=1\n",
        "        time.sleep(5)\n",
        "        urls = urls + getURLs(sitemap, driver)\n",
        "    driver.quit()\n",
        "    return urls"
      ],
      "metadata": {
        "id": "o2wAlwh2b5_V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "BF_urls = BFURLs(halfway, verbose=True)"
      ],
      "metadata": {
        "id": "W6o7Fzq9cGYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('BF_urls.txt', 'w') as f:\n",
        "    #for url in halfway:\n",
        "        #f.write(url)\n",
        "    f.write('\\n'.join(halfway))"
      ],
      "metadata": {
        "id": "34MBXmy7cKjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Valor**"
      ],
      "metadata": {
        "id": "AmdI_n_gaADu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap = 'https://valor.globo.com/sitemap/valor/sitemap.xml'\n",
        "\n",
        "driver.get(sitemap)\n",
        "\n",
        "response_html = driver.page_source\n",
        "\n",
        "soupHtml = BeautifulSoup(response_html)"
      ],
      "metadata": {
        "id": "ZtBYz1mKcPj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dontInclude = ['elections',\n",
        "               'author',\n",
        "               'elections2020',\n",
        "               'category',\n",
        "               'autorPost',\n",
        "               'post_tag',\n",
        "               'page-sitemap',\n",
        "               'programming']"
      ],
      "metadata": {
        "id": "FHAzuHeJcWC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halfway = soupHtml.find_all('loc')\n",
        "halfway = [i.text for i in halfway if 'http' in i.text]\n",
        "print(len(halfway))\n",
        "halfway[0:5]"
      ],
      "metadata": {
        "id": "COjB0R4FcX1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap_urls = []\n",
        "for url in halfway:\n",
        "    dummy = []\n",
        "    for dont in dontInclude:\n",
        "        if dont in str(url):\n",
        "            dummy.append(dont)\n",
        "    if dummy == []:\n",
        "        sitemap_urls.append(url)"
      ],
      "metadata": {
        "id": "w79JzOsOcaRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getURLs(sitemap_url, driver):\n",
        "    driver.get(sitemap_url)\n",
        "    response_html = driver.page_source\n",
        "    soupHtml = BeautifulSoup(response_html)\n",
        "    urls = soupHtml.find_all('loc')\n",
        "    return [t.text for t in urls if 'https' in t.text]"
      ],
      "metadata": {
        "id": "ctGO1KVKf10-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def valorURLs(sitemap_urls, verbose=False):\n",
        "    urls = []\n",
        "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "    if verbose:\n",
        "        total = len(sitemap_urls)\n",
        "        progress = 1\n",
        "    for sitemap in sitemap_urls:\n",
        "        if verbose:\n",
        "            print(f'Progress - {round((progress/total)*100,2)}%')\n",
        "            progress+=1\n",
        "        time.sleep(5)\n",
        "        urls = urls + getURLs(sitemap, driver)\n",
        "    driver.quit()\n",
        "    return urls\n",
        "\n"
      ],
      "metadata": {
        "id": "kZDuUJVgca3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valor = valorURLs(sitemap_urls, verbose=True)"
      ],
      "metadata": {
        "id": "p5jGXuNLcdcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('valor_urls.txt', 'w') as f:\n",
        "    for url in valor :\n",
        "        f.write(url)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "TksHdlA0ci40"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CNN**"
      ],
      "metadata": {
        "id": "iE3uN_ewaHi8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap = 'https://www.cnnbrasil.com.br//sitemap_index.xml'\n",
        "driver.get(sitemap)\n",
        "response_html = driver.page_source\n",
        "soupHtml = BeautifulSoup(response_html)\n",
        "\n",
        "dontInclude = ['elections',\n",
        "               'author',\n",
        "               'elections2020',\n",
        "               'category',\n",
        "               'autorPost',\n",
        "               'post_tag',\n",
        "               'page-sitemap',\n",
        "               'programming']"
      ],
      "metadata": {
        "id": "QKllb0vRcqil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "halfway = soupHtml.find_all('td')\n",
        "halfway = [i.text for i in halfway if 'https' in i.text]\n",
        "print(len(halfway))\n",
        "halfway[0:5]"
      ],
      "metadata": {
        "id": "Q6wscXQUc3wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sitemap_urls = []\n",
        "for url in halfway:\n",
        "    dummy = []\n",
        "    for dont in dontInclude:\n",
        "        if dont in str(url):\n",
        "            dummy.append(dont)\n",
        "    if dummy == []:\n",
        "        sitemap_urls.append(url)"
      ],
      "metadata": {
        "id": "S0-q80yGc5Q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def getURLs(sitemap_url, driver):\n",
        "    driver.get(sitemap_url)\n",
        "    response_html = driver.page_source\n",
        "    soupHtml = BeautifulSoup(response_html)\n",
        "    urls = soupHtml.find_all('td')\n",
        "    return [t.text for t in urls if 'http' in t.text]"
      ],
      "metadata": {
        "id": "WoLyJbNtfPdJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def cnnURLs(sitemap_urls, verbose=False):\n",
        "    urls = []\n",
        "    driver = webdriver.Chrome('chromedriver', chrome_options=chrome_options)\n",
        "    if verbose:\n",
        "        total = len(sitemap_urls)\n",
        "        progress = 1\n",
        "    for sitemap in sitemap_urls:\n",
        "        if verbose:\n",
        "            print(f'Progress - {round((progress/total)*100,2)}%')\n",
        "            progress+=1\n",
        "        time.sleep(5)\n",
        "        urls = urls + getURLs(sitemap, driver)\n",
        "    driver.quit()\n",
        "    return urls"
      ],
      "metadata": {
        "id": "tXlkTaabc6iU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = cnnURLs(sitemap_urls, verbose=True)"
      ],
      "metadata": {
        "id": "CIUYz_ltc-0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('cnn_urls.txt', 'w') as f:\n",
        "    for url in cnn:\n",
        "        f.write(url)\n",
        "        f.write('\\n')"
      ],
      "metadata": {
        "id": "R2IIkh5IdBRc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Scrapers**"
      ],
      "metadata": {
        "id": "qrhO69BgdMKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle ###\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import requests \n",
        "from bs4 import BeautifulSoup\n",
        "import gdown ###\n",
        "import time ###"
      ],
      "metadata": {
        "id": "OGjfc8r3gTEF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Article():\n",
        "\n",
        "    def __init__(self, author=\"\", date=\"\", source=\"\", title=\"\", subtitle=\"\", text=\"\"):\n",
        "        self.author = author\n",
        "        self.date = date\n",
        "        self.source = source\n",
        "        self.title = title\n",
        "        self.subtitle = subtitle\n",
        "        self.text = text\n",
        "\n",
        "\n",
        "def save_pickle_file(to_save, file_name):\n",
        "    ''' fileName example: listaElementos.pkl '''\n",
        "    with open(file_name, \"wb\") as f:\n",
        "        pickle.dump(to_save, f)\n",
        "\n",
        "\n",
        "def load_pickle_file(file_path):\n",
        "    with open(file_path, \"rb\") as f:\n",
        "        to_read = pickle.load(f)\n",
        "    return to_read\n",
        "\n",
        "\n",
        "def read_urls(file_name):\n",
        "    with open(file_name, encoding=\"utf8\") as file:\n",
        "        lines = file.readlines()\n",
        "        lines = [line.rstrip() for line in lines]\n",
        "    return lines\n",
        "\n",
        "\n",
        "def scrape_urls(urls_list, scrapeFunction, batch_size, timeout):\n",
        "    urls_contents = []\n",
        "    for index in tqdm(range(0, len(urls_list), batch_size)):\n",
        "        batch = urls_list[index : index + batch_size]\n",
        "        with concurrent.futures.ThreadPoolExecutor() as executor:\n",
        "            scraped_urls = [executor.submit(scrapeFunction, url, timeout) for url in batch]\n",
        "            concurrent.futures.wait(scraped_urls)\n",
        "            urls_contents = urls_contents + [query.result() for query in scraped_urls]\n",
        "        for art in urls_contents[-5:]: ###\n",
        "            print(art['title']) ###\n",
        "    return urls_contents"
      ],
      "metadata": {
        "id": "mWODjFhBgXCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "def num_without_title(fname):\n",
        "    pkl_file = load_pickle_file(fname)\n",
        "    counter = 0\n",
        "    for art in pkl_file:\n",
        "        if art.title == '':\n",
        "          counter+=1\n",
        "    print(f'\\n Number articles without title = {counter}.\\n')"
      ],
      "metadata": {
        "id": "9pJWbulmgahF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Jornal do Comércio**"
      ],
      "metadata": {
        "id": "60htU8UQdSc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def JC_scraper(url, timeout=5):\n",
        "    try:\n",
        "        page = requests.get(url, timeout=timeout)\n",
        "        parsed_html = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "        art_header= parsed_html.find('div',{'class': 'article-header'})\n",
        "    # Timeout exception\n",
        "    except:\n",
        "        parsed_content = {\n",
        "            'title': '',\n",
        "            'subtitle': '',\n",
        "            'author': '',\n",
        "            'date': '',\n",
        "            'category': '',\n",
        "            'text': ''\n",
        "        }\n",
        "\n",
        "        return parsed_content\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        title = art_header.find('h1', {'class': 'article-title'})\n",
        "        title = title.text\n",
        "    except:\n",
        "        title = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        subtitle = art_header.find('div', {'class': 'content'})\n",
        "        subtitle = subtitle.find('p')\n",
        "        subtitle = subtitle.text\n",
        "    except:\n",
        "        subtitle = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        text = ''\n",
        "        toAppend = ' '\n",
        "        filtered_divs = parsed_html.find('div', {'id': 'textContentCount'})\n",
        "        vc= filtered_divs.find('div', {'class':\"alsoread\"})\n",
        "        if vc!= None:\n",
        "          filtered_divs.find('div', {'class':\"alsoread\"}).decompose()\n",
        "        paragraphs = filtered_divs.find_all('p')\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.text\n",
        "            text += paragraph + toAppend\n",
        "    except:\n",
        "        text = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        date = parsed_html.find('span', {'class': 'infotitle'})\n",
        "        date = date.text\n",
        "    except:\n",
        "        date = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        category = parsed_html.find('li', {'class': 'item-cat'})\n",
        "        category = category.text\n",
        "    except:\n",
        "        category = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        author = parsed_html.find('h5', {'class': 'author'})\n",
        "        author = author.text.strip()\n",
        "    except:\n",
        "        author = ''\n",
        "    \n",
        "\n",
        "    \n",
        "    parsed_content = {\n",
        "        'title': title,\n",
        "        'subtitle': subtitle,\n",
        "        'author': author,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    return parsed_content"
      ],
      "metadata": {
        "id": "Ms5eua6agdlX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "JC=JC_scraper('https://jc.ne10.uol.com.br/mundo/2022/03/14957730-petroleo-mantem-alta-puxado-por-embargo-dos-eua-a-russia.html')\n",
        "\n",
        "print(JC['title'])\n",
        "print(JC['subtitle'])\n",
        "print(JC['author'])\n",
        "print(JC['date'])\n",
        "print(JC['text'])"
      ],
      "metadata": {
        "id": "0i7Vq2S8glJN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "urls = read_urls(\"jc-urls.txt\")\n",
        "urls_batches = [urls[i:5000+i] for i in range(0, 300000, 5000)]\n",
        "ulrs_batches = urls_batches[72:]"
      ],
      "metadata": {
        "id": "146y4B7bgkU1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "for uBatch, i in zip(urls_batches, range(len(urls_batches))):\n",
        "    scraped_articles = scrape_urls(uBatch, JC_scraper, batch_size=100, timeout=5)\n",
        "        \n",
        "    JC_articles = [\n",
        "        Article(author=article['author'],\n",
        "        date=article['date'],\n",
        "        source=\"Jornal do Comercio\", \n",
        "        title=article['title'], \n",
        "        subtitle=article['subtitle'], \n",
        "        text=article['text']) for article in scraped_articles]\n",
        "\n",
        "    pkl_name = f'JC-articles-{i+72}.pkl'\n",
        "    save_pickle_file(JC_articles, pkl_name)\n",
        "    num_without_title(pkl_name)\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "id": "qnD9fEukgoEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Brasil de Fato**"
      ],
      "metadata": {
        "id": "5kx3P_mqdSkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4.element import PreformattedString\n",
        "def BF_scraper(url, timeout=5):\n",
        "\n",
        "    try:\n",
        "        page = requests.get(url, timeout=timeout)\n",
        "        parsed_html = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "        parsed_html = parsed_html.find('main',{'class': 'grid-4-columns'})\n",
        "        art_header= parsed_html.find('header',{'class': None})\n",
        "        post=parsed_html.find('div',{'class':'content'})\n",
        "        #Timeout exception\n",
        "    except:\n",
        "        \n",
        "        parsed_content = {\n",
        "            'title': '',\n",
        "            'subtitle': '',\n",
        "            'author': '',\n",
        "            'date': '',\n",
        "            'category': '',\n",
        "            'text': ''\n",
        "        }\n",
        "\n",
        "        return parsed_content\n",
        "\n",
        "\n",
        "    try:\n",
        "        title = art_header.find('h1', {'class': 'title'})\n",
        "        title = title.text\n",
        "    except:\n",
        "        title = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        subtitle = art_header.find('h2', {'class': 'description'})\n",
        "        subtitle = subtitle.text\n",
        "    except:\n",
        "        subtitle = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "      text = ''\n",
        "      toAppend = ' '\n",
        "    \n",
        "      paragraphs =[]\n",
        "      post1=post.find('div',{'class':'text-content'})   \n",
        "      p_tags=post1.find_all('p')\n",
        "      for tag in p_tags:\n",
        "        if tag.find('a')!=None:\n",
        "          x=tag.find('a').decompose()\n",
        "          \n",
        "        paragraphs.append(tag.text)\n",
        "   \n",
        "      for paragraph in paragraphs:\n",
        "\n",
        "        paragraph = paragraph\n",
        "        text += paragraph + toAppend\n",
        "    except:\n",
        "      text=''  \n",
        "\n",
        "\n",
        "    try:\n",
        "        author = art_header.find('div',{'class':'author'})\n",
        "        \n",
        "        author = author.text.strip()\n",
        "    except:\n",
        "        author = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        \n",
        "        date = art_header.find('time',{'class': 'date' })\n",
        "        date = date.text\n",
        "    except:\n",
        "        date = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        category = post.find('li', {'class': 'item-cat'})\n",
        "        category = category.text\n",
        "    except:\n",
        "        category = ''\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    parsed_content = {\n",
        "        'title': title,\n",
        "        'subtitle': subtitle,\n",
        "        'author': author,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    return parsed_content\n",
        "\n"
      ],
      "metadata": {
        "id": "z_tw26ZxgzgN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "BF=BF_scraper('https://www.brasildefato.com.br/2022/03/25/fundo-controlado-por-principe-amigo-de-bolsonaro-cresce-com-crise-no-brasil')\n",
        "\n",
        "print(BF['title'])\n",
        "print(BF['subtitle'])\n",
        "print(BF['author'])\n",
        "print(BF['date'])\n",
        "print(BF['text'])"
      ],
      "metadata": {
        "id": "0Lf703R-gz_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "urls = read_urls(\"brasildefato-urls.txt\")\n",
        "urls_batches = [urls[i:5000+i] for i in range(0, 35000, 5000)]"
      ],
      "metadata": {
        "id": "A9bxrb7Rg2Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "for uBatch, i in zip(urls_batches, range(len(urls_batches))):\n",
        "    scraped_articles = scrape_urls(uBatch, BF_scraper, batch_size=100, timeout=5)\n",
        "        \n",
        "    BF_articles = [\n",
        "        Article(author=article['author'],\n",
        "        date=article['date'],\n",
        "        source=\"Brasil de Fato\", \n",
        "        title=article['title'], \n",
        "        subtitle=article['subtitle'], \n",
        "        text=article['text']) for article in scraped_articles]\n",
        "\n",
        "    pkl_name = f'BF-articles-{i}.pkl'\n",
        "    save_pickle_file(BF_articles, pkl_name)\n",
        "    num_without_title(pkl_name)\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "id": "x6kXFTpAg8J9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Valor**"
      ],
      "metadata": {
        "id": "HxmNbHyvdSoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valor_scraper(url, timeout=5):\n",
        "\n",
        "  \n",
        "    try:\n",
        "        page = requests.get(url, timeout=timeout)\n",
        "        parsed_html = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "        art_header= parsed_html.find('div',{'class': 'article-header'})\n",
        "\n",
        "    except:\n",
        "        parsed_content = {\n",
        "            'title': '',\n",
        "            'subtitle': '',\n",
        "            'author': '',\n",
        "            'date': '',\n",
        "            'category': '',\n",
        "            'text': ''\n",
        "        }\n",
        "\n",
        "        return parsed_content\n",
        "\n",
        "\n",
        "    try:\n",
        "        title = parsed_html.find('h1', {'class': 'content-head__title'})\n",
        "        title = title.text\n",
        "    except:\n",
        "        title = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        subtitle = parsed_html.find('div', {'class': 'medium-centered subtitle'})\n",
        "        subtitle = subtitle.find('h2', {'class': 'content-head__subtitle'})\n",
        "        subtitle = subtitle.text\n",
        "    except:\n",
        "        subtitle = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        text = ''\n",
        "        toAppend = ' '\n",
        "        filtered_divs = parsed_html.find('div', {'class': 'mc-article-body'})\n",
        "\n",
        "        paragraphs = filtered_divs.find_all('p',{'class': 'content-text__container'})\n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.text\n",
        "            text += paragraph + toAppend\n",
        "    except:\n",
        "        text = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        date = parsed_html.find('p', {'class': 'content-publication-data__updated'})\n",
        "        date = date.text\n",
        "    except:\n",
        "        date = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        category = parsed_html.find('li', {'class': 'item-cat'})\n",
        "        category = category.text\n",
        "    except:\n",
        "        category = ''\n",
        "\n",
        "\n",
        "\n",
        "    try:\n",
        "        author = parsed_html.find('p', {'class': 'content-publication-data__from'})\n",
        "        author = author.text.strip()\n",
        "    except:\n",
        "        author = ''\n",
        "    \n",
        "\n",
        "    parsed_content = {\n",
        "        'title': title,\n",
        "        'subtitle': subtitle,\n",
        "        'author': author,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    return parsed_content\n"
      ],
      "metadata": {
        "id": "GPBJVNQqhDIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V=valor_scraper('https://valor.globo.com/empresas/noticia/2022/03/21/cppib-do-grupo-de-controle-da-aliansce-eleva-participacao-na-br-malls-para-1024percent.ghtml')\n",
        "\n",
        "print(V['title'])\n",
        "print(V['subtitle'])\n",
        "print(V['author'])\n",
        "print(V['date'])\n",
        "print(V['text'])"
      ],
      "metadata": {
        "id": "iqkIlV54hE-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "urls = read_urls(\"valor-urls.txt\")\n",
        "urls_batches = [urls[i:5000+i] for i in range(0, 300000, 5000)]"
      ],
      "metadata": {
        "id": "fU5g0bmLhGmt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "for uBatch, i in zip(urls_batches, range(len(urls_batches))):\n",
        "    scraped_articles = scrape_urls(uBatch, valor_scraper, batch_size=100, timeout=5)\n",
        "        \n",
        "    valor_articles = [\n",
        "        Article(author=article['author'],\n",
        "        date=article['date'],\n",
        "        source=\"Valor\", \n",
        "        title=article['title'], \n",
        "        subtitle=article['subtitle'], \n",
        "        text=article['text']) for article in scraped_articles]\n",
        "\n",
        "    pkl_name = f'valor-articles-{i}.pkl'\n",
        "    save_pickle_file(valor_articles, pkl_name)\n",
        "    num_without_title(pkl_name)\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "id": "bsDQBX0mhJb0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**CNN**"
      ],
      "metadata": {
        "id": "Rr8TEVMfdS8z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4.element import PreformattedString\n",
        "def CNN_scraper(url, timeout=5):\n",
        "    try:\n",
        "        page = requests.get(url, timeout=timeout)\n",
        "        parsed_html = BeautifulSoup(page.text, features=\"html.parser\")\n",
        "        art_header= parsed_html.find('header',{'class': 'post__header'})\n",
        "        post=parsed_html.find('div',{'class': 'post__content'})\n",
        "\n",
        "    except:\n",
        "        parsed_content = {\n",
        "            'title': '',\n",
        "            'subtitle': '',\n",
        "            'author': '',\n",
        "            'date': '',\n",
        "            'category': '',\n",
        "            'text': ''\n",
        "        }\n",
        "\n",
        "        return parsed_content\n",
        "\n",
        "\n",
        "    try:\n",
        "        title = art_header.find('h1', {'class': 'post__title'})\n",
        "        title = title.text\n",
        "    except:\n",
        "        title = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        subtitle = art_header.find('p', {'class': 'post__excerpt'})\n",
        "        subtitle = subtitle.text\n",
        "    except:\n",
        "        subtitle = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        text = ''\n",
        "        toAppend = ' '\n",
        "        \n",
        "        post.find('div',{'class':'riddle_target'}).decompose()\n",
        " \n",
        "        paragraphs = post.find_all('p')\n",
        "        \n",
        "        for paragraph in paragraphs:\n",
        "            paragraph = paragraph.text\n",
        "            text += paragraph + toAppend\n",
        "    except:\n",
        "        text = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        date = art_header.find('span', {'class': 'post__data'})\n",
        "        date = date.text\n",
        "    except:\n",
        "        date = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        category = parsed_html.find('li', {'class': 'item-cat'})\n",
        "        category = category.text\n",
        "    except:\n",
        "        category = ''\n",
        "\n",
        "\n",
        "    try:\n",
        "        author = art_header.find('a',)\n",
        "        author = author.text.strip()\n",
        "    except:\n",
        "        author = ''\n",
        "    \n",
        "    \n",
        "    parsed_content = {\n",
        "        'title': title,\n",
        "        'subtitle': subtitle,\n",
        "        'author': author,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'text': text\n",
        "    }\n",
        "\n",
        "    return parsed_content\n",
        "\n"
      ],
      "metadata": {
        "id": "WZo5_YYBhOts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CNN=CNN_scraper('https://www.cnnbrasil.com.br/business/mercados-hoje-24-marco-2022/')\n",
        "\n",
        "print(CNN['title'])\n",
        "print(CNN['subtitle'])\n",
        "print(CNN['author'])\n",
        "print(CNN['date'])\n",
        "print(CNN['text'])\n"
      ],
      "metadata": {
        "id": "tYlkd1BEhO2j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "urls = read_urls(\"cnn_urls.txt\")\n",
        "urls_batches = [urls[i:5000+i] for i in range(0, 300000, 5000)]"
      ],
      "metadata": {
        "id": "UgykpAVRhO-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "###\n",
        "for uBatch, i in zip(urls_batches, range(len(urls_batches))):\n",
        "    scraped_articles = scrape_urls(uBatch, CNN_scraper, batch_size=100, timeout=5)\n",
        "        \n",
        "    CNN_articles = [\n",
        "        Article(author=article['author'],\n",
        "        date=article['date'],\n",
        "        source=\"CNN\", \n",
        "        title=article['title'], \n",
        "        subtitle=article['subtitle'], \n",
        "        text=article['text']) for article in scraped_articles]\n",
        "\n",
        "    pkl_name = f'cnn-articles-{i}.pkl'\n",
        "    save_pickle_file(CNN_articles, pkl_name)\n",
        "    num_without_title(pkl_name)\n",
        "    time.sleep(60)"
      ],
      "metadata": {
        "id": "8YWpslMIhPTV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
